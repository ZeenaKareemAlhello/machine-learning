{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfff9413",
   "metadata": {},
   "source": [
    "Fashion Classification Project (PyTorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ac7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97442e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_dataset_path = \"../../data/clothing-dataset-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b282fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428d187",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de622065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\n",
    "    \"../../data/clothing-dataset-small/train/pants/0098b991-e36e-4ef1-b5ee-4154b21e2a92.jpg\"\n",
    ")\n",
    "\n",
    "img = img.resize((224, 224))\n",
    "\n",
    "X = np.array(img)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942ce4e",
   "metadata": {},
   "source": [
    "# Use Pre-trained model\n",
    "\n",
    "ready models will use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8242105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec953557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "# will use the model for evaluation not traing it\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cefac9",
   "metadata": {},
   "source": [
    "# pre-processing and Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "126e4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet expects 224Ã—224 RGB images, normalized with ImageNet mean/std\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        # image resize and crop it\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        # something like np.array(img)\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf82534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocess(img)\n",
    "print(x.shape)\n",
    "\n",
    "# send one batch of image\n",
    "batch_t = torch.unsqueeze(x, 0)\n",
    "batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7944b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2427e+00, -2.0114e+00, -1.2542e-01, -6.6945e-01,  1.7635e+00,\n",
       "         -1.3329e+00, -1.4026e+00, -1.3484e+00, -1.4840e+00, -4.0130e+00,\n",
       "         -6.5940e+00, -5.4901e+00, -5.5818e+00, -3.5736e+00, -2.6900e+00,\n",
       "         -5.0894e+00, -4.4860e+00, -4.8563e-01, -1.8529e+00, -3.4865e+00,\n",
       "         -4.7337e+00, -1.4860e+00, -1.4399e+00, -7.9737e-01, -3.8429e+00,\n",
       "         -2.3196e+00, -1.0433e+00, -1.8372e+00, -1.1772e+00, -9.1635e-01,\n",
       "         -1.8537e+00, -1.5561e+00, -5.9366e-01,  8.1879e-01,  2.9164e+00,\n",
       "         -1.7834e+00, -1.6100e+00, -2.0272e+00, -1.6498e+00, -1.4771e+00,\n",
       "         -2.3772e+00, -3.0200e+00, -1.4916e+00, -9.4792e-01, -1.3502e+00,\n",
       "         -1.4371e+00, -3.0592e+00, -1.8795e+00, -1.7403e+00, -2.2762e+00,\n",
       "         -1.9235e+00,  4.0033e-02,  1.1634e+00, -1.9422e+00,  2.4749e-01,\n",
       "         -3.4308e+00, -1.2632e+00, -1.6105e+00, -1.0462e+00, -7.5241e-01,\n",
       "          5.5334e-01, -6.2872e-01, -1.7878e+00, -1.0621e+00, -1.6738e+00,\n",
       "         -1.1067e+00,  2.0722e-02, -1.0550e+00, -1.8519e+00, -8.2732e-01,\n",
       "         -8.5814e-01, -1.7861e-01, -3.3456e+00, -1.2958e+00, -2.3580e+00,\n",
       "         -1.8882e+00,  5.9178e-01, -2.9763e+00,  4.9708e-02, -4.3246e-01,\n",
       "         -1.3569e-01, -2.6347e+00, -2.7850e+00, -2.2857e+00, -4.7305e+00,\n",
       "         -2.3409e+00, -2.6051e+00,  1.7601e+00,  1.7837e-01, -9.9600e-01,\n",
       "         -3.4192e+00, -9.4682e-02, -3.4255e+00, -2.6194e+00, -5.6604e+00,\n",
       "         -3.3795e+00, -1.7685e+00, -5.1270e+00, -5.0949e+00, -3.4569e+00,\n",
       "         -3.9164e+00, -5.4107e-01,  9.9300e-01, -8.2324e-01, -1.8903e+00,\n",
       "         -3.2565e+00, -2.6470e-01, -2.8123e+00, -1.8395e+00, -3.4037e+00,\n",
       "         -1.3363e+00,  6.3510e-01,  1.6910e+00, -2.0931e+00,  6.7629e-02,\n",
       "         -4.7336e+00, -1.5756e+00, -1.8438e+00,  6.1067e-01,  1.3665e+00,\n",
       "          8.3474e-01,  6.4846e-01,  1.3500e+00, -1.7382e-01,  6.6758e-01,\n",
       "         -2.0405e-01, -6.2327e-01, -2.4733e+00, -3.1353e+00, -3.2459e+00,\n",
       "         -4.0903e+00, -2.5577e-02, -1.7798e+00, -3.9044e+00, -2.7480e+00,\n",
       "         -4.1276e+00, -1.0598e+00, -2.0087e+00, -3.0090e+00, -4.7384e+00,\n",
       "         -3.6227e+00, -4.4977e+00, -4.1757e+00, -1.6108e+00, -1.7636e+00,\n",
       "          1.1822e-01, -7.5913e-01,  1.4761e+00, -9.1700e-01,  2.6160e-01,\n",
       "         -2.4285e+00,  2.4988e+00,  4.4099e-01,  3.1010e+00,  1.9837e+00,\n",
       "          1.3858e+00,  8.0050e-01,  1.3088e+00,  1.7227e+00,  8.1435e-01,\n",
       "          2.0295e+00,  5.7359e-01,  1.2768e+00,  1.8882e+00,  1.1271e+00,\n",
       "          1.5341e+00,  7.7018e-01,  1.7137e-01,  6.7683e-01,  2.5036e+00,\n",
       "          2.3382e+00,  2.0650e+00,  2.2976e+00,  2.0489e+00,  1.9267e+00,\n",
       "          8.0016e-01,  7.8249e-01,  2.8812e+00,  1.8410e+00,  1.4451e+00,\n",
       "          4.9418e-01,  3.4085e-01,  2.4595e+00,  2.5748e+00,  1.0590e+00,\n",
       "          1.2521e+00,  1.6247e+00,  2.2825e+00,  3.4828e-01,  1.4586e+00,\n",
       "          2.5222e-01,  8.2975e-01,  2.3965e+00,  1.8507e+00,  1.3772e-01,\n",
       "          2.3988e+00,  1.8610e+00,  1.9909e+00,  1.6852e+00,  1.3545e+00,\n",
       "          6.8995e-01,  4.9182e-01,  1.8955e+00,  1.7183e+00,  2.1383e+00,\n",
       "          6.4743e-01,  5.1589e-01, -6.8328e-02,  1.2457e+00,  8.0037e-01,\n",
       "          1.3171e+00,  1.7905e+00,  1.0044e+00,  4.7831e-01,  7.5634e-01,\n",
       "          2.2725e+00,  6.4706e-01,  1.7038e+00, -1.1188e+00,  1.1208e-01,\n",
       "          9.1953e-01, -3.8261e-01,  2.0354e+00,  3.6516e+00,  2.2777e+00,\n",
       "          1.3098e+00,  6.6234e-01,  1.0674e+00,  9.9493e-01,  1.3119e+00,\n",
       "          1.1496e+00,  1.2429e-01,  1.3383e+00,  3.4362e+00,  1.1830e+00,\n",
       "          1.3084e+00,  1.3980e+00,  2.9006e+00,  1.5519e+00,  6.4357e-01,\n",
       "          8.7854e-01,  1.6062e-01,  1.6743e+00,  3.2196e-01,  2.9629e-01,\n",
       "          5.8944e-01,  1.9685e+00,  1.8104e-01,  1.7858e+00,  1.8235e+00,\n",
       "          2.3866e+00,  2.0966e+00,  1.5919e+00,  2.6529e-01,  1.9578e+00,\n",
       "          1.4256e+00,  2.0952e+00,  2.6149e+00,  2.9650e+00,  2.3064e+00,\n",
       "          2.8005e-01,  1.6739e+00,  1.2777e+00,  1.4980e+00,  1.6438e+00,\n",
       "          2.3678e+00,  1.2980e+00,  8.2014e-01,  7.1205e-01, -3.0323e+00,\n",
       "         -1.2144e+00, -3.4723e+00, -1.6535e+00, -4.9095e-01, -3.3074e+00,\n",
       "         -3.1848e+00, -1.6636e+00, -2.1862e+00, -3.4174e+00, -9.5152e-01,\n",
       "         -1.8341e+00,  2.1928e-02, -7.3064e-01, -5.7064e-02,  3.5497e-01,\n",
       "          9.8029e-01, -2.8137e+00, -1.0012e+00, -3.4435e+00, -4.5259e+00,\n",
       "         -3.5155e+00, -4.1100e+00, -2.6733e+00, -2.5621e+00, -3.6446e+00,\n",
       "         -2.2558e+00, -1.7731e+00, -1.8959e+00, -9.2872e-01,  3.3639e-01,\n",
       "         -3.8249e+00, -1.5686e+00, -1.8790e+00, -1.2960e+00, -1.8952e+00,\n",
       "         -1.7587e+00, -4.1569e-01, -2.9310e+00, -2.9879e+00, -2.1334e+00,\n",
       "         -1.3112e+00, -2.2758e+00, -1.1152e+00,  1.2218e+00, -3.0944e-02,\n",
       "         -7.0920e-01, -8.2950e-01,  1.6561e-01, -1.1196e+00, -2.3772e+00,\n",
       "         -2.8556e+00, -1.6595e+00, -1.8913e+00, -2.6004e+00, -2.1655e+00,\n",
       "         -1.7932e+00, -1.8504e+00,  2.6342e-01, -3.0465e-01, -3.1352e-01,\n",
       "         -7.9652e-01, -1.3696e+00,  3.8884e-01, -2.5024e+00, -1.4655e+00,\n",
       "         -3.4491e+00, -2.7909e+00, -2.3790e+00,  2.3456e-01, -2.7921e+00,\n",
       "         -3.1157e+00, -4.6368e-01, -2.4228e+00, -2.9289e+00, -4.5614e+00,\n",
       "         -2.1257e+00, -3.0225e+00, -3.2835e+00, -3.2047e+00, -4.6112e+00,\n",
       "         -4.6023e+00, -3.8296e+00, -4.1667e+00, -3.7222e+00, -1.3287e+00,\n",
       "         -2.4616e+00,  1.1529e+00, -2.7705e-01,  2.4891e+00,  1.9075e+00,\n",
       "         -3.9332e+00, -2.1770e-01, -1.8006e+00, -3.9753e-01, -1.5067e+00,\n",
       "         -2.7931e+00, -3.3272e+00, -9.2316e-01, -2.2215e+00, -2.4695e+00,\n",
       "         -1.7614e+00, -1.1000e+00, -1.5836e+00, -1.3215e+00, -3.8806e+00,\n",
       "         -4.1669e+00, -3.9933e+00, -2.1368e+00, -4.5247e-01, -2.4093e+00,\n",
       "         -1.8335e+00, -1.7336e+00, -1.6988e+00, -3.1257e+00, -2.6982e+00,\n",
       "         -1.4905e+00, -3.1299e+00, -4.8198e+00, -4.6874e+00,  3.6436e+00,\n",
       "         -2.4972e-01,  2.3852e+00, -3.4809e+00, -3.0642e+00,  2.0108e+00,\n",
       "          2.9079e+00, -4.1423e+00, -3.8025e+00, -1.5163e+00,  6.2196e+00,\n",
       "          2.9551e+00, -9.4780e-02, -2.6104e-04, -1.6748e+00, -2.5831e+00,\n",
       "         -6.4761e-01, -9.4581e-01, -2.7898e-01, -2.3706e+00,  3.6032e-01,\n",
       "         -1.8722e+00,  6.8161e+00,  8.7262e-03, -1.9635e-01,  6.2906e+00,\n",
       "         -1.0507e+00,  2.0627e+00,  1.5991e-01,  1.2433e+00,  4.0523e+00,\n",
       "          3.6490e-01,  1.3287e+00, -6.0668e-02,  1.1369e+00,  1.4078e+00,\n",
       "         -4.2938e+00, -2.3500e+00, -2.3147e+00, -1.3990e-03, -8.4728e-01,\n",
       "         -1.6488e+00,  6.9739e+00,  1.1587e+00,  2.1465e+00,  6.1431e+00,\n",
       "          5.4475e+00,  1.7709e-01,  1.1019e+00, -2.6932e-01,  2.1252e+00,\n",
       "         -1.5372e+00, -2.2949e+00,  5.7100e-02,  2.7272e+00,  2.7814e-01,\n",
       "          2.1031e+00, -2.2264e-01,  2.3602e+00, -1.7148e+00, -4.2321e+00,\n",
       "          1.9559e+00,  3.7922e+00,  6.7855e+00, -3.5920e-01, -6.5320e-01,\n",
       "         -7.2654e-01,  2.0421e+00,  7.8847e+00, -4.1007e-01,  3.8435e+00,\n",
       "          1.9233e-01,  2.3730e+00,  1.8433e+00,  2.8684e+00,  5.4726e+00,\n",
       "          4.7554e+00,  1.0797e+00, -9.4360e-01, -1.1603e+00,  2.5678e-01,\n",
       "          7.0441e-01, -1.9396e+00, -1.5571e+00,  3.6700e+00,  9.3037e+00,\n",
       "         -1.1584e+00, -2.9628e+00,  2.6593e+00,  7.8499e-01,  5.0536e-01,\n",
       "          4.2192e+00, -1.5465e+00,  2.7696e-02, -4.3071e+00,  1.9221e+00,\n",
       "          2.9620e-01, -1.4872e+00,  2.7081e+00, -5.3327e-01, -2.8626e+00,\n",
       "          3.5256e+00,  1.1273e-01, -1.2913e-01,  6.5253e-01, -1.0463e+00,\n",
       "         -9.1929e-01,  1.4608e+00, -2.6282e+00, -2.6454e+00,  2.9023e+00,\n",
       "         -1.4848e+00,  8.3918e+00,  5.0245e+00, -8.6512e-01,  2.3873e-01,\n",
       "          1.6682e-01,  9.9055e-01, -8.2332e-01, -1.6412e-01, -3.8482e+00,\n",
       "         -1.0479e+00,  6.3858e-01,  3.3857e+00,  1.6685e+00,  3.9796e+00,\n",
       "          5.1652e+00,  6.5340e+00, -9.9558e-01,  2.4241e+00,  8.5569e-01,\n",
       "          2.9637e+00, -3.2206e-01,  8.1575e-01,  5.6956e+00,  1.5088e+00,\n",
       "         -2.1598e+00, -1.4633e-01, -5.2114e-01, -2.9397e-01,  4.3003e+00,\n",
       "         -3.0130e-01,  1.2256e+00, -6.2412e-01,  7.9652e-01,  2.1154e+00,\n",
       "          4.5080e-02, -3.3221e+00,  2.4097e-01, -9.9938e-01,  1.4913e+00,\n",
       "         -6.1975e-01,  5.4529e-01,  1.0017e+00,  1.8557e+00,  1.1754e+00,\n",
       "          1.2003e+00,  2.0525e+00, -3.8140e+00, -1.3092e+00,  5.4728e-01,\n",
       "         -1.0405e+00, -1.0627e+00,  3.7670e+00,  6.7741e-01, -1.1816e-02,\n",
       "         -4.8951e+00,  8.6739e-01,  1.6781e+00,  2.3107e+00,  2.7877e+00,\n",
       "         -7.7274e-01, -3.3450e+00, -4.7563e+00,  1.0841e+00,  4.4812e-01,\n",
       "         -4.3470e+00,  1.6520e-01, -5.0606e-01,  6.9163e+00, -1.3631e+00,\n",
       "          6.1034e+00, -1.4508e+00, -4.8845e-01, -2.3354e+00, -2.0162e-01,\n",
       "         -2.9345e+00, -2.7225e+00, -1.2119e+00,  6.2763e+00,  1.2386e+00,\n",
       "         -3.0504e+00, -3.3867e+00, -1.1453e+00,  4.2761e-01,  2.4286e+00,\n",
       "          2.8232e+00, -1.7988e+00,  2.4833e+00,  5.0981e+00,  4.9737e+00,\n",
       "          1.5021e+00,  5.6893e+00, -3.6246e+00,  1.8332e+00,  1.9302e+00,\n",
       "         -3.2784e+00,  1.8904e+00,  3.4448e+00, -2.1293e+00, -2.2716e+00,\n",
       "          3.1500e+00,  8.0120e+00,  2.4944e+00, -3.8787e+00, -1.2414e+00,\n",
       "          3.5868e-01,  5.0283e+00,  3.7540e-01,  1.0037e+01, -1.3501e+00,\n",
       "          6.9468e+00, -6.8595e-01, -1.2484e+00,  7.4791e-01,  5.0962e+00,\n",
       "          3.4005e+00,  1.0488e+00,  6.7886e+00,  1.0408e+00,  5.3509e-01,\n",
       "          2.5929e-02, -1.8374e+00, -9.2486e-01,  1.9927e+00, -4.2812e-01,\n",
       "         -2.0253e+00, -7.0303e-01,  2.9854e+00,  1.5677e-01,  7.4555e-01,\n",
       "          5.7354e+00,  1.8295e+00,  3.6413e-01, -1.4148e-01, -1.2864e+00,\n",
       "         -1.1003e+00,  5.5149e+00, -1.6526e+00,  5.1981e+00,  3.2627e+00,\n",
       "         -1.5131e+00,  1.4819e+00,  8.7406e-01,  4.1161e+00, -1.4704e+00,\n",
       "         -2.0326e+00,  1.8513e+00,  1.2846e-01,  1.3448e+00, -9.9329e-01,\n",
       "          9.7455e-01, -5.3260e-01,  3.6259e+00, -7.4536e-01, -5.8646e-01,\n",
       "          7.3372e+00,  1.3935e+00, -4.0923e-01,  5.3471e+00,  1.2459e+00,\n",
       "         -2.4180e+00, -3.0752e+00, -6.9395e-01, -3.4106e+00,  2.8385e-01,\n",
       "         -7.2416e-01, -7.6817e-01,  3.2357e+00, -2.7264e+00,  2.0028e+00,\n",
       "         -1.0633e+00, -1.5880e+00,  4.5712e+00, -2.8324e-01,  1.6977e+00,\n",
       "          5.7549e-01,  2.6615e+00,  1.5571e+00,  6.6370e+00,  1.6649e+00,\n",
       "          4.0641e+00,  1.3770e+00, -2.4653e+00,  2.1027e+00,  9.7938e-01,\n",
       "         -2.1565e+00, -2.0564e+00, -2.7305e-02,  9.1111e-01,  8.9473e+00,\n",
       "         -3.8159e+00,  5.0738e+00,  8.6892e-01, -3.6036e-01, -1.6231e+00,\n",
       "         -1.5910e+00, -2.7346e-01,  7.0123e+00, -3.3823e+00,  9.2247e-01,\n",
       "          2.2722e+00,  2.6125e-01,  3.1594e+00, -1.2694e+00, -1.1991e+00,\n",
       "         -2.7826e+00, -1.1954e+00, -5.3147e-01,  4.0108e-02,  6.1944e-01,\n",
       "         -1.5395e+00,  1.1038e+00, -6.7880e-01,  2.8215e+00, -3.9381e-01,\n",
       "          3.1366e+00, -9.3718e-01, -1.5296e+00, -8.4256e-01,  1.2238e-01,\n",
       "         -5.1435e-02,  1.5010e+00, -1.3960e+00, -1.0896e+00,  1.7919e-01,\n",
       "         -3.9551e-02, -4.3765e-02, -1.6157e+00,  4.2320e+00,  5.1950e-01,\n",
       "         -1.6836e+00,  4.2517e+00, -2.8509e-01,  3.4719e-01, -2.0957e+00,\n",
       "          7.6209e+00,  6.7638e-01, -1.5036e+00,  3.9436e-01,  2.0696e+00,\n",
       "          1.4134e+00,  2.0238e+00,  2.6312e-01, -1.4200e+00, -7.0587e-01,\n",
       "          2.0130e-01, -1.4113e+00,  1.2089e+00,  2.8254e+00,  4.4622e+00,\n",
       "          2.5999e+00, -1.8954e+00,  3.8846e-01,  1.6910e+00, -1.1858e+00,\n",
       "          1.3644e-01, -3.0592e-01, -3.7680e-01,  1.2286e+00,  9.8731e-02,\n",
       "          1.3138e+00,  1.4046e+00, -2.7908e+00,  6.7710e-01,  1.9152e+00,\n",
       "          1.7744e+00,  1.4949e-01, -5.0755e-01, -1.9224e+00,  1.0614e+00,\n",
       "          2.6619e+00,  1.8976e+00,  3.5843e+00, -8.2801e-01,  3.5344e+00,\n",
       "          7.2987e+00, -5.3709e-02,  3.6826e+00,  8.0971e-01, -2.4188e+00,\n",
       "          4.3515e+00, -2.3024e+00,  1.2587e+00,  1.2555e+00,  2.0114e+00,\n",
       "          3.7424e+00,  1.2925e+00,  5.7158e-01,  2.0053e+00, -3.5824e-01,\n",
       "          3.9624e+00,  1.2747e+00,  2.5195e+00,  5.5069e+00,  3.2238e+00,\n",
       "          3.4936e-01,  6.4130e+00,  6.4228e+00, -2.3224e-01,  1.0566e-01,\n",
       "         -2.8079e+00,  2.2533e+00, -1.4772e+00, -2.7824e+00,  1.8898e+00,\n",
       "         -7.1699e-01,  3.5270e+00,  5.9634e-01,  4.6936e+00,  1.5689e-02,\n",
       "         -1.2647e+00,  2.2867e+00, -1.3056e+00,  2.2175e+00, -1.5349e+00,\n",
       "         -4.6180e+00,  1.2828e+00, -1.7366e+00,  1.8779e-01,  2.3572e-01,\n",
       "         -4.6437e+00, -2.9081e+00,  1.2750e+00,  3.9733e+00,  8.6479e+00,\n",
       "         -2.4064e+00, -1.3854e+00,  5.7593e-01,  1.8969e+00, -1.6161e+00,\n",
       "          4.3346e+00,  2.6685e+00, -2.7259e+00, -1.5337e+00,  9.9304e+00,\n",
       "         -3.4078e-01,  3.3654e+00,  4.4242e+00,  3.3693e+00, -1.5025e+00,\n",
       "          5.3267e+00,  9.3998e+00,  6.9188e+00,  1.7036e+00,  1.7286e+00,\n",
       "          1.7361e+00, -3.9000e-01, -4.4480e+00,  3.0078e-01, -2.0831e+00,\n",
       "          1.5331e+00,  2.1793e+00,  9.8245e-01, -5.3088e-01,  2.7401e-01,\n",
       "          1.6719e-02, -2.5556e+00, -1.6504e+00, -1.9620e+00,  1.5246e+00,\n",
       "         -1.3135e+00,  2.8445e+00,  1.1022e+00, -1.7916e+00, -2.3621e+00,\n",
       "         -7.5396e-01, -2.0879e+00, -3.3149e+00,  6.9817e-01,  8.6440e+00,\n",
       "          2.1773e-01,  1.1370e+00,  2.4950e+00, -2.9784e+00, -4.4536e+00,\n",
       "          8.7632e-01,  3.8753e+00,  1.7165e-01, -1.2970e+00,  2.2442e+00,\n",
       "          1.4119e+00,  1.1793e+00,  2.5968e+00,  4.5429e-01, -1.6285e+00,\n",
       "          8.3691e+00, -1.8360e+00,  4.0503e+00, -3.7879e+00,  8.8597e-01,\n",
       "         -2.1211e+00, -1.4843e+00,  4.0586e-01, -4.0050e-02,  5.1268e+00,\n",
       "         -1.2856e+00,  8.9448e-01,  3.8534e+00,  9.6675e-01,  1.7775e+00,\n",
       "         -3.0262e+00,  4.4616e-01,  2.4768e-01,  5.3782e+00,  9.7801e-01,\n",
       "          9.9040e-01,  8.4939e+00, -8.6252e-01, -5.8575e-01, -1.7384e+00,\n",
       "          8.6170e-01,  7.8071e+00, -2.8178e+00,  5.3193e-02,  3.8206e+00,\n",
       "          3.9469e-01,  1.2139e-01, -8.8168e-01,  5.3958e-02, -1.5826e+00,\n",
       "         -3.5658e+00,  1.2359e+00, -3.2264e+00, -8.7659e-03, -6.4372e-01,\n",
       "         -2.0142e+00, -4.4258e+00, -3.0973e+00, -1.9725e+00,  2.5011e+00,\n",
       "         -7.3076e-02, -1.3254e+00,  4.9985e-02, -2.0431e+00,  6.1508e-01,\n",
       "         -1.1432e+00,  1.8243e+00,  4.4477e-01, -8.7622e-01,  9.8277e-02,\n",
       "         -3.7507e-01, -7.3709e-01, -6.1071e-01, -1.6221e+00, -2.4976e+00,\n",
       "         -1.7389e+00, -4.9326e+00, -1.9972e+00, -1.2423e+00, -1.9929e+00,\n",
       "         -5.9892e-01, -2.4031e+00, -1.8811e+00, -3.1361e-01,  1.3907e+00,\n",
       "         -1.9640e+00, -5.3765e-01, -2.7596e+00, -2.6951e+00, -2.6865e+00,\n",
       "         -1.1398e+00,  4.0050e-01, -1.5128e+00, -2.6929e+00,  3.5428e-02,\n",
       "          1.3723e+00, -4.4911e-01, -1.0672e+00, -1.8628e+00, -2.3395e+00,\n",
       "         -6.7870e-01, -1.8638e+00,  8.5472e-01, -3.4719e+00, -8.7304e-01,\n",
       "         -8.5125e-01, -6.1703e-02,  2.9646e+00,  5.2929e-01, -2.5677e+00,\n",
       "         -1.0899e+00,  1.3511e+00,  2.8431e+00, -1.7325e+00, -2.8385e+00,\n",
       "         -2.5390e+00, -5.3996e+00, -8.3618e-01, -3.7191e+00, -3.7573e+00,\n",
       "         -4.8876e-01, -3.2382e+00, -3.3441e+00, -3.1689e+00, -2.7007e+00,\n",
       "         -2.0286e+00, -9.4778e-01, -2.3986e+00, -6.5312e-01,  2.6655e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no_grad() mean use model to make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(batch_t)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70bf3a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c69d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[608, 834, 841, 474, 689, 824, 869, 906, 501, 885, 601, 457, 911, 735,\n",
       "         655, 775, 697, 431, 610, 842, 568, 411, 617, 452, 678, 516, 797, 796,\n",
       "         414, 578, 399, 434, 570, 630, 523, 591, 636, 793, 464, 435, 903, 658,\n",
       "         840, 638, 515, 894, 588, 614, 691, 606, 502, 589, 465, 808, 672, 749,\n",
       "         837, 780, 830, 529, 731, 728, 480, 643, 680, 419, 887, 514, 823, 790,\n",
       "         876, 897, 459, 914, 451, 552, 785, 777, 473, 223, 389, 652, 772, 774,\n",
       "         806, 490, 597, 233, 615, 512, 838, 836, 639, 667, 794, 702, 600, 715,\n",
       "         153, 627, 258, 977, 520, 400,  34, 395, 499, 237, 177, 463, 861, 982,\n",
       "         748, 585, 713, 559, 443, 487, 831, 999, 770, 676, 477, 257, 750, 882,\n",
       "         183, 792, 169, 929, 151, 872, 602, 358, 587, 182, 584, 518, 195, 192,\n",
       "         250, 391, 461, 265, 447, 170, 558, 259, 172, 811, 187, 224, 215, 700,\n",
       "         801, 879,   0, 813, 851, 433, 204, 439, 534, 445, 683, 251, 256, 739,\n",
       "         171, 416, 546, 173, 456, 222, 160, 741, 784, 394, 788, 669, 623, 197,\n",
       "         154, 246, 254, 450, 594, 174, 484, 764, 359, 771, 828, 202, 596, 804,\n",
       "         163, 196, 543, 646, 193, 462, 178, 593, 631, 936, 249, 211, 248, 899,\n",
       "         765,   4,  87, 845, 844, 158, 203, 217, 843, 674, 753, 112, 198, 557,\n",
       "         242, 261, 513, 679, 264, 186, 252, 677, 238, 165, 850, 859, 524, 590,\n",
       "         721, 263, 539, 641, 147, 496, 189, 179, 255, 740, 880, 424, 761, 236,\n",
       "         656, 954, 155, 681, 965, 119, 199, 981, 122, 648, 232, 421, 210, 760,\n",
       "         229, 225, 157, 235, 266, 786, 816, 262, 162, 822, 791, 782, 783, 185,\n",
       "         659, 208, 418, 579, 921, 758, 531, 313, 747, 545, 234, 881, 544,  52,\n",
       "         432, 356, 230, 871, 423, 164, 711, 862, 437, 563, 466, 227, 769, 184,\n",
       "         616, 618, 212, 542, 228, 102, 506, 905, 852, 285, 684, 904, 650, 898,\n",
       "         699, 220, 688, 896, 889, 240, 875, 642, 692, 556, 910, 519, 972, 120,\n",
       "         191, 267,  33, 522, 159, 778, 156, 209, 175, 533, 478, 176, 166, 214,\n",
       "         613, 629, 268, 470, 868, 200, 553, 763, 168, 736, 124, 226, 493, 121,\n",
       "         205, 216, 239, 511, 111, 709, 934, 118, 807,  76, 245, 827, 675, 161,\n",
       "         787,  60, 549, 541, 619, 978, 729, 206, 479, 180, 201, 213, 883, 564,\n",
       "         901, 937, 152, 583, 892, 961, 915, 738, 332, 752, 607, 420, 632, 409,\n",
       "         605, 284, 795, 188, 733, 181, 299, 243, 848, 244, 485, 664, 260, 444,\n",
       "         854, 253, 327, 742, 149, 701, 469, 190, 902,  54, 537, 504, 819, 338,\n",
       "         870, 745, 460, 818, 247, 724,  88, 436, 877, 167, 505, 317, 566, 241,\n",
       "         417, 628, 766, 194, 755, 647, 231, 719, 916, 145, 491, 219, 799, 759,\n",
       "         939, 114, 442, 918, 913, 932,  78, 535, 708,  51, 964, 482, 620, 281,\n",
       "          66, 855, 809, 412, 402, 428, 923, 554, 131, 687, 314, 725, 893, 726,\n",
       "         720, 776, 283, 422, 976, 207, 930,  91, 401,   2, 492,  80, 633, 526,\n",
       "         508, 123,  71, 413, 574, 125, 361, 446, 798, 390, 106, 438, 696, 357,\n",
       "         407, 673, 732, 528, 530, 328, 756, 329, 953, 521, 835, 789, 453, 693,\n",
       "         940, 757, 221, 846, 714, 363, 657, 458, 306, 624,  79, 966, 378, 341,\n",
       "          17, 572, 990, 273, 567, 767, 527, 853, 707, 651, 488, 956, 101, 908,\n",
       "         654,  32, 950, 942, 540, 126, 532,  61, 924, 405, 998, 454,   3, 970,\n",
       "         712, 611, 662, 626, 744, 315, 805, 665, 455, 282, 941, 653,  59, 865,\n",
       "         146, 666, 560, 330,  23, 103, 507,  69, 773, 316, 987, 718, 429, 975,\n",
       "          70, 907, 503, 974, 938, 917,  29, 148, 495, 367, 622, 298, 716, 467,\n",
       "         406, 996,  43, 279, 649, 517,  89, 538, 287, 550,  26,  58, 494, 510,\n",
       "         415,  67, 136,  63, 551, 670, 967, 723, 980, 371, 635,  65, 312, 218,\n",
       "         318, 960, 935, 582, 475, 468,  28, 754, 706, 704, 577, 270, 604, 948,\n",
       "         612,  56, 810, 703, 895, 634,  73, 303, 878, 812, 548, 310, 860, 373,\n",
       "         931, 354,   5, 110,   7, 609,  44, 569, 331, 826, 722,   6, 746, 743,\n",
       "          45,  22, 571, 334, 644,  39, 802,   8, 891, 500,  21, 486, 385,  42,\n",
       "         839, 737, 364, 962, 640, 398, 717, 833, 814, 440, 710, 481,  31, 472,\n",
       "         301, 116, 919, 372, 671, 695,  36,  57, 143, 727, 829, 943, 694, 884,\n",
       "         430,  38, 857, 637, 272, 321, 276,  64, 403, 730, 382, 448, 983, 381,\n",
       "         817, 909, 945,  48, 305, 370, 144,  96, 296, 132,  35,  62, 863, 325,\n",
       "         586, 362, 380, 280, 886,  27, 621, 108, 117, 326,  68,  18,  30, 968,\n",
       "         971, 410, 302,  47, 952,  75, 104, 322, 304, 751, 297, 768,  50, 471,\n",
       "          53, 858, 955, 928, 949, 947, 137,   1, 925, 625,  37, 995, 645, 933,\n",
       "         686, 849, 866, 113, 734, 890, 345, 598, 309, 377, 685, 525, 324, 277,\n",
       "         368, 295, 599, 311,  49,  83, 441, 781, 427,  25, 573, 969,  85, 426,\n",
       "          74, 864, 408, 319,  40, 337, 997, 951, 825, 379, 660, 779, 342, 150,\n",
       "         355, 682, 369, 127, 944, 333, 985, 856, 293, 979, 404, 323,  86,  93,\n",
       "         497,  81, 498, 292, 959,  14, 963, 958, 384, 994, 576, 832, 668, 134,\n",
       "         957, 803, 705,  82, 762, 336, 339, 365, 800, 107, 286, 912, 984, 320,\n",
       "         489, 821, 343, 307, 575, 476,  77, 873, 308, 138,  41, 346, 900, 269,\n",
       "         580,  46, 393, 661, 927, 340, 383, 386, 128, 993, 275, 348, 922, 991,\n",
       "         129, 105, 595, 347, 274, 867, 536, 366, 992, 561,  72,  95, 698, 581,\n",
       "         109, 663, 278,  90,  92,  55, 288, 335,  99, 973, 271, 392,  19, 290,\n",
       "         920,  13, 140, 592, 294, 988, 353, 989, 888, 397, 547, 690, 300, 351,\n",
       "          24, 509, 603, 374, 133, 100, 360, 376,   9, 130, 291, 135, 396, 352,\n",
       "         375, 142, 449, 425, 483, 565, 926, 847, 874,  16, 141, 289, 344, 350,\n",
       "         349, 815, 820, 388,  84, 115,  20, 139, 562, 387, 555, 946,  15,  98,\n",
       "          97, 986,  11,  12,  94,  10]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes sorts by predictions\n",
    "_, indices = torch.sort(output, descending=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9169d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "1: jean\n",
      "2: suit\n",
      "3: sweatshirt\n",
      "4: cardigan\n",
      "5: overskirt\n"
     ]
    }
   ],
   "source": [
    "# !curl -o imagenet_classes.txt \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "\n",
    "# Load ImageNet class names\n",
    "with open(\"../../data/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top5_indices = indices[0, :5].tolist()\n",
    "top5_classes = [categories[i] for i in top5_indices]\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, class_name in enumerate(top5_classes):\n",
    "    print(f\"{i+1}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0aa433",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "take pre-trained model and tune it to work as we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60030683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996c5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "input_size = 224\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ca13056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ClothingDataset(\n",
    "    data_dir=f\"{clothing_dataset_path}/train\", transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = ClothingDataset(\n",
    "    data_dir=f\"{clothing_dataset_path}/validation\", transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d0915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d430ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "# 1.\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ClothingClassifierMobileNet(num_classes=10)\n",
    "model.to(device)\n",
    "# lr: learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fd8f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model, optimizer, train_loader, val_loader, criterion, num_epochs, device\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        #! Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Iterate over the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to prevent accumulation\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training loss\n",
    "            running_loss += loss.item()\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        #! Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # Disable gradient calculation for validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move data to the specified device (GPU or CPU)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate validation loss\n",
    "                val_loss += loss.item()\n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # Update total and correct predictions\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce638315",
   "metadata": {},
   "source": [
    "7. Model Checkpointing\n",
    "   save the weight for the best model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "247fd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the train function\n",
    "def train_and_evaluate_with_checkpointing(\n",
    "    model, optimizer, train_loader, val_loader, criterion, num_epochs, device\n",
    "):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #! Training phase\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Iterate over the training data\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the specified device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to prevent accumulation\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training loss\n",
    "            running_loss += loss.item()\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        #! Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # Disable gradient calculation for validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data\n",
    "            for inputs, labels in val_loader:\n",
    "                # Move data to the specified device (GPU or CPU)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate validation loss\n",
    "                val_loss += loss.item()\n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                # Update total and correct predictions\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Checkpoint the model if validation accuracy improved\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f\"mobilenet_v2_{epoch+1:02d}_{val_acc:.3f}.pth\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028fea3",
   "metadata": {},
   "source": [
    "# 6. Tuning the Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b7dd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Learning Rate\n",
    "\n",
    "\n",
    "def make_model(learning_rate=0.01):\n",
    "    model = ClothingClassifierMobileNet(num_classes=10)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf838441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Learning Rate: 0.001 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Learning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     model, optimizer = make_model(learning_rate=lr)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtrain_and_evaluate_with_checkpointing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# best learning rate is 0.001\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_and_evaluate_with_checkpointing\u001b[39m\u001b[34m(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\u001b[39m\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[32m     25\u001b[39m loss = criterion(outputs, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mClothingClassifierMobileNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.global_avg_pooling(x)\n\u001b[32m     28\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py:64\u001b[39m, in \u001b[36mInvertedResidual.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.conv(x)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.0001, 0.01, 0.1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n=== Learning Rate: {lr} ===\")\n",
    "    model, optimizer = make_model(learning_rate=lr)\n",
    "    train_and_evaluate_with_checkpointing(\n",
    "        model, optimizer, train_loader, val_loader, criterion, num_epochs, device\n",
    "    )\n",
    "# best learning rate is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d6121",
   "metadata": {},
   "source": [
    "8. Adding Inner Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, size_inner=100):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        #! New inner layer\n",
    "        self.inner = nn.Linear(1280, size_inner)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #!\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61168f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.001, size_inner=100):\n",
    "    model = ClothingClassifierMobileNet(num_classes=10, size_inner=size_inner)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76274b",
   "metadata": {},
   "source": [
    "# Dropout Regularization\n",
    "\n",
    "when apply regu ,we need to train the model for more time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "635860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ClothingClassifierMobileNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_inner=100,\n",
    "        droprate=0.2,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super(ClothingClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # Load pre-trained MobileNetV2\n",
    "        self.base_model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # Add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.inner = nn.Linear(1280, size_inner)\n",
    "        self.relu = nn.ReLU()\n",
    "        #! dropout\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  #! Apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58737f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.001, size_inner=100, droprate=0.2):\n",
    "    model = ClothingClassifierMobileNet(\n",
    "        num_classes=10, size_inner=size_inner, droprate=droprate\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21a750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Drop rate: 0.0 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.2536, Train Acc: 0.5900\n",
      "  Val Loss: 0.8520, Val Acc: 0.6569\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.7201, Train Acc: 0.7673\n",
      "  Val Loss: 0.6514, Val Acc: 0.8065\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.5795, Train Acc: 0.8103\n",
      "  Val Loss: 0.6253, Val Acc: 0.7918\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.5303, Train Acc: 0.8266\n",
      "  Val Loss: 0.5775, Val Acc: 0.8035\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.4803, Train Acc: 0.8289\n",
      "  Val Loss: 0.5563, Val Acc: 0.8094\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.4167, Train Acc: 0.8621\n",
      "  Val Loss: 0.5400, Val Acc: 0.8006\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.3875, Train Acc: 0.8680\n",
      "  Val Loss: 0.5502, Val Acc: 0.8152\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.3605, Train Acc: 0.8778\n",
      "  Val Loss: 0.5820, Val Acc: 0.7947\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.3499, Train Acc: 0.8807\n",
      "  Val Loss: 0.5828, Val Acc: 0.8065\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.3338, Train Acc: 0.8787\n",
      "  Val Loss: 0.5514, Val Acc: 0.8152\n",
      "\n",
      "=== Drop rate: 0.2 ===\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.4423, Train Acc: 0.5218\n",
      "  Val Loss: 0.9127, Val Acc: 0.6979\n",
      "Epoch 2/10\n",
      "  Train Loss: 0.8496, Train Acc: 0.7278\n",
      "  Val Loss: 0.6806, Val Acc: 0.7595\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.6984, Train Acc: 0.7735\n",
      "  Val Loss: 0.5998, Val Acc: 0.8123\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.6040, Train Acc: 0.8074\n",
      "  Val Loss: 0.5884, Val Acc: 0.8006\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.5577, Train Acc: 0.8100\n",
      "  Val Loss: 0.5491, Val Acc: 0.8182\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.5155, Train Acc: 0.8230\n",
      "  Val Loss: 0.5889, Val Acc: 0.7889\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.4781, Train Acc: 0.8308\n",
      "  Val Loss: 0.5573, Val Acc: 0.7977\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Drop rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m model, optimizer = make_model(droprate=dr)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\u001b[39m\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[32m     25\u001b[39m loss = criterion(outputs, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mClothingClassifierMobileNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.global_avg_pooling(x)\n\u001b[32m     35\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py:64\u001b[39m, in \u001b[36mInvertedResidual.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.conv(x)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "droprates = [0.0, 0.2, 0.5, 0.8]\n",
    "\n",
    "for dr in droprates:\n",
    "    print(f\"\\n=== Drop rate: {dr} ===\")\n",
    "    model, optimizer = make_model(droprate=dr)\n",
    "    train_and_evaluate(\n",
    "        model, optimizer, train_loader, val_loader, criterion, num_epochs, device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47016b15",
   "metadata": {},
   "source": [
    "# 10. Data Augmentation\n",
    "\n",
    "Data Augmentation artificially increases dataset size by applying random transformations to training images.\n",
    "\n",
    "Important rules:\n",
    "âœ… Apply ONLY to training data\n",
    "âŒ Never augment validation/test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06ed6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms WITH augmentation\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),  # Rotate up to 10 degrees\n",
    "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontal flip\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation transforms - NO augmentation, same as before\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2520c42",
   "metadata": {},
   "source": [
    "# 11. Using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41819b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: mobilenet_v2_07_0.821.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ClothingClassifierMobileNet:\n\tMissing key(s) in state_dict: \"inner.weight\", \"inner.bias\". \n\tsize mismatch for output_layer.weight: copying a param with shape torch.Size([10, 1280]) from checkpoint, the shape in current model is torch.Size([10, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m model = ClothingClassifierMobileNet(size_inner=\u001b[32m32\u001b[39m, droprate=\u001b[32m0.2\u001b[39m, num_classes=\u001b[32m10\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m model.to(device)\n\u001b[32m     13\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/DataTalks-zoomcamp/machine-learning/.venvtorch/lib/python3.11/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ClothingClassifierMobileNet:\n\tMissing key(s) in state_dict: \"inner.weight\", \"inner.bias\". \n\tsize mismatch for output_layer.weight: copying a param with shape torch.Size([10, 1280]) from checkpoint, the shape in current model is torch.Size([10, 32])."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Find best checkpoint\n",
    "list_of_files = glob.glob('mobilenet_v2_*.pth')\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "print(f\"Loading model from: {latest_file}\")\n",
    "\n",
    "# Load model\n",
    "model = ClothingClassifierMobileNet(size_inner=32, droprate=0.2, num_classes=10)\n",
    "# print(model)\n",
    "model.load_state_dict(torch.load(latest_file))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446370e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "from keras_image_helper import create_preprocessor\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_pytorch_style(X):\n",
    "    # X: shape (1, 224, 224, 3), dtype=float32, values in [0, 255]\n",
    "    X = X / 255.0\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n",
    "    \n",
    "    # Convert NHWC â†’ NCHW (batch, height, width, channels â†’ batch, channels, height, width)\n",
    "    X = X.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    # Normalize\n",
    "    X = (X - mean) / std\n",
    "    \n",
    "    return X.astype(np.float32)\n",
    "\n",
    "preprocessor = create_preprocessor(preprocess_pytorch_style, target_size=(224, 224))\n",
    "\n",
    "# Predict from URL\n",
    "url = 'http://bit.ly/mlbookcamp-pants'\n",
    "X = preprocessor.from_url(url)\n",
    "X = torch.Tensor(X).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X).cpu().numpy()[0]\n",
    "\n",
    "classes = [\n",
    "    \"dress\", \"hat\", \"longsleeve\", \"outwear\", \"pants\",\n",
    "    \"shirt\", \"shoes\", \"shorts\", \"skirt\", \"t-shirt\"\n",
    "]\n",
    "\n",
    "result = dict(zip(classes, pred.tolist()))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
