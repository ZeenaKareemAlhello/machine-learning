{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"../../data/raw/churn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == \"object\"].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "df.totalcharges = pandas.to_numeric(df.totalcharges, errors=\"coerce\")\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == \"yes\").astype(int)\n",
    "#\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "y_test = df_test.churn.values\n",
    "\n",
    "del df_train[\"churn\"]\n",
    "del df_val[\"churn\"]\n",
    "del df_test[\"churn\"]\n",
    "#\n",
    "numerical = [\"tenure\", \"monthlycharges\", \"totalcharges\"]\n",
    "\n",
    "categorical = [\n",
    "    \"gender\",\n",
    "    \"seniorcitizen\",\n",
    "    \"partner\",\n",
    "    \"dependents\",\n",
    "    \"phoneservice\",\n",
    "    \"multiplelines\",\n",
    "    \"internetservice\",\n",
    "    \"onlinesecurity\",\n",
    "    \"onlinebackup\",\n",
    "    \"deviceprotection\",\n",
    "    \"techsupport\",\n",
    "    \"streamingtv\",\n",
    "    \"streamingmovies\",\n",
    "    \"contract\",\n",
    "    \"paperlessbilling\",\n",
    "    \"paymentmethod\",\n",
    "]\n",
    "\n",
    "#\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#\n",
    "val_dict = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "churn_decision = y_pred >= 0.5\n",
    "(y_val == churn_decision).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "the fraction of correct preditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_val == churn_decision).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(1132 / 1409)\n",
    "\n",
    "# or\n",
    "print((y_val == churn_decision).mean())\n",
    "\n",
    "# or\n",
    "accuracy_score(y_val, churn_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best value of decision cond.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "thresholds = numpy.linspace(0, 1, 21)\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    churn_decision = y_pred >= t\n",
    "    score = accuracy_score(y_val, churn_decision)\n",
    "    print(\"%.2f %.3f\" % (t, score))\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(thresholds, scores)\n",
    "# the best threshold is 0.50 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy model\n",
    "from collections import Counter\n",
    "\n",
    "Counter(y_pred >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Confusion Table\n",
    "\n",
    "evaluate the quality of the model by different strategies.\n",
    "\n",
    "When comes to a prediction of an LR model, each falls into one of four different categories:\n",
    "\n",
    "Prediction is that the customer WILL churn. This is known as the Positive class\n",
    "And Customer actually churned - Known as a True Positive (TP)\n",
    "But Customer actually did not churn - Known as a False Positive (FP)\n",
    "Prediction is that the customer WILL NOT churn' - This is known as the Negative class\n",
    "Customer did not churn - True Negative (TN)\n",
    "Customer churned - False Negative (FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# people who are going to churn\n",
    "actual_positive = y_val == 1\n",
    "# people who are not going to churn\n",
    "actual_negative = y_val == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "predict_positive = y_pred >= t\n",
    "predict_negative = y_pred < t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_positive & actual_positive\n",
    "\n",
    "tp = (predict_positive & actual_positive).sum()\n",
    "print(tp)\n",
    "tn = (predict_negative & actual_negative).sum()\n",
    "print(tn)\n",
    "\n",
    "fp = (predict_positive & actual_negative).sum()\n",
    "print(fp)\n",
    "fn = (predict_negative & actual_positive).sum()\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = numpy.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(confusion_matrix / confusion_matrix.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Precision and Recall\n",
    "\n",
    "(eval. matrix)\n",
    "\n",
    "Precision : From the predicted positives, how many we predicted right.\n",
    "\n",
    "Recall : From the real positives, how many we predicted right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = tp / (tp + fp)\n",
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp)\n",
    "print(tp + fp)\n",
    "\n",
    "# 210/311\n",
    "# explain: 67% are correct,33% are mistiks(we pred them to churn but they are not churned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp)\n",
    "print(tp + fn)\n",
    "\n",
    "# 210/386\n",
    "# explain: 54% are correct,46% are mistiks(we pred them to not churn but they are churned)\n",
    "# (so the accuracy 80% was misleading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "\n",
    "ROC curves consider Recall and FPR under all the possible thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpr:true positive rate\n",
    "tpr = tp / (tp + fn)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr: false positive rate\n",
    "fpr = fp / (fp + tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr_dataframe(y_val, y_pred):\n",
    "    scores = []\n",
    "    thresholds = numpy.linspace(0, 1, 101)\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = y_val == 1\n",
    "        actual_negative = y_val == 0\n",
    "\n",
    "        predict_positive = y_pred >= t\n",
    "        predict_negative = y_pred < t\n",
    "\n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, tn, fp, fn))\n",
    "\n",
    "    scores\n",
    "\n",
    "    columns = [\"threshold\", \"tp\", \"tn\", \"fp\", \"fn\"]\n",
    "    df_scores = pandas.DataFrame(scores, columns=columns)\n",
    "    return df_scores\n",
    "\n",
    "\n",
    "df_scores = tpr_fpr_dataframe(y_val, y_pred)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores[\"tpr\"] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "df_scores[\"fpr\"] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores[\"tpr\"], label=\"TPR\")\n",
    "plt.plot(df_scores.threshold, df_scores[\"fpr\"], label=\"FPR\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Random model\n",
    "\n",
    "baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "numpy.random.seed(1)\n",
    "y_rand = numpy.random.uniform(0, 1, size=len(y_val))\n",
    "y_rand.round(3)\n",
    "\n",
    "# Accuracy for our random model is around 50%\n",
    "((y_rand >= 0.5) == y_val).mean()\n",
    "\n",
    "random_model_fpr, random_model_tpr, random_model_threshold = roc_curve(y_val, y_rand)\n",
    "\n",
    "plt.plot(random_model_threshold, random_model_tpr, label=\"TPR\")\n",
    "plt.plot(random_model_threshold, random_model_fpr, label=\"FPR\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Ideal model\n",
    "(makes correct predictions for every example)\n",
    "\n",
    "1. order the pred from lowest to highest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = (y_val == 0).sum()\n",
    "num_pos = (y_val == 1).sum()\n",
    "num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ideal = numpy.repeat([0, 1], [num_neg, num_pos])\n",
    "y_ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ideal_pred = numpy.linspace(0, 1, len(y_ideal))\n",
    "y_ideal_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ideal = ((y_ideal_pred >= 0.726) == y_ideal).mean()\n",
    "accuracy_ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_fpr, ideal_tpr, ideal_threshold = roc_curve(y_ideal, y_ideal_pred)\n",
    "\n",
    "plt.plot(ideal_threshold, ideal_tpr, label=\"TPR\")\n",
    "plt.plot(ideal_threshold, ideal_fpr, label=\"FPR\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Putting everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores[\"tpr\"], label=\"TPR\")\n",
    "plt.plot(df_scores.threshold, df_scores[\"fpr\"], label=\"FPR\")\n",
    "\n",
    "# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\n",
    "# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\n",
    "\n",
    "plt.plot(ideal_threshold, ideal_tpr, label=\"ideal_TPR\", color=\"black\")\n",
    "plt.plot(ideal_threshold, ideal_fpr, label=\"ideal_FPR\", color=\"green\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Roc curve\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(df_scores.fpr, df_scores.tpr, label=\"model\")\n",
    "plt.plot([0, 1], [0, 1], label=\"random\")\n",
    "# plt.plot(df_rand.fpr, df_rand.tpr, label='random')\n",
    "plt.plot(ideal_fpr, ideal_tpr, label=\"ideal\")\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# ROC AUC (measure the performance of the model)\n",
    "\n",
    "(calculate the area under the curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "auc(df_scores.fpr, df_scores.tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(ideal_fpr, ideal_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC interpretation\n",
    "# (AUC tells us the probability that a randomly selected positive example has a score that is higher than a randomly selected negative example)\n",
    "\n",
    "neg = y_pred[y_val == 0]\n",
    "pos = y_pred[y_val == 1]\n",
    "\n",
    "n = 50000\n",
    "\n",
    "numpy.random.seed(1)\n",
    "pos_ind = numpy.random.randint(0, len(pos), size=n)\n",
    "neg_ind = numpy.random.randint(0, len(neg), size=n)\n",
    "pos[pos_ind] > neg[neg_ind]\n",
    "\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation\n",
    "(eval. matrix)\n",
    "(Evaluating the same model on different subsets of data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return dv, model\n",
    "\n",
    "\n",
    "dv, model = train(df_train, y_train)\n",
    "\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient=\"records\")\n",
    "\n",
    "    X = dv.fit_transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_pred = predict(df_val, dv, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "kfold.split(df_full_train)\n",
    "\n",
    "train_idx, val_idx = next(kfold.split(df_full_train))\n",
    "print(len(train_idx), len(val_idx))\n",
    "\n",
    "print(len(df_full_train))\n",
    "\n",
    "# We can use iloc to select a part of this dataframe\n",
    "df_train = df_full_train.iloc[train_idx]\n",
    "df_val = df_full_train.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in tqdm(kfold.split(df_full_train)):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can utilize the scores generated to compute the average score across the 10 folds, with a standard deviation .\n",
    "\n",
    "print(\"%.3f +- %.3f\" % (numpy.mean(scores), numpy.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "# Parameter Tuning\n",
    "(regularization parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model\n",
    "\n",
    "dv, model = train(df_train, y_train, C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C cannot be 0.0 will cause error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)  \n",
    " \n",
    "for C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n",
    "     \n",
    "    scores = []\n",
    " \n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    " \n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    " \n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    " \n",
    "    print('C=%s %.3f +- %.3f' % (C, numpy.mean(scores), numpy.std(scores)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
